{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03864cbc",
   "metadata": {},
   "source": [
    "## Implementing the Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa641ae9",
   "metadata": {},
   "source": [
    "### Contributors:\n",
    "Raphael Steinborn |\n",
    "Lidia Makishti |\n",
    "Vineeth Racharla |\n",
    "Saqib Sarwar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67299d04",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "Vineeth Racharla 349073 | Saqib Sarwar 351757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba10f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcce6331-b59c-4eb0-9978-8b455547e1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Training on:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ace16",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "Raphael Steinborn 351421 | Lidia Makishti 346621 | Vineeth Racharla 349073 | Saqib Sarwar 351757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09d938cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\Users\\Saqib\n",
      "[nltk_data]     Sarwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Saqib\n",
      "[nltk_data]     Sarwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download and preprocess the text8 dataset\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "words = list(reuters.words())\n",
    "text = ' '.join(words)\n",
    "\n",
    "# Tokenize the text\n",
    "words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Counter(words)\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "# Subsampling of frequent words\n",
    "def subsample_frequent_words(words, threshold=1e-5):\n",
    "    total_count = len(words)\n",
    "    word_counts = Counter(words)\n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    prob_drop = {word: 1 - np.sqrt(threshold / freqs[word]) for word in word_counts}\n",
    "    subsampled_words = [word for word in words if random.random() < (1 - prob_drop[word])]\n",
    "    return subsampled_words\n",
    "\n",
    "words = subsample_frequent_words(words)\n",
    "\n",
    "# Generate training data\n",
    "def generate_training_data(words, word_to_idx, window_size):\n",
    "    data = []\n",
    "    for i in range(len(words)):\n",
    "        target = word_to_idx[words[i]]\n",
    "        context = []\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j != 0 and 0 <= i + j < len(words):\n",
    "                context.append(word_to_idx[words[i + j]])\n",
    "        for context_word in context:\n",
    "            data.append((target, context_word))\n",
    "    return data\n",
    "\n",
    "window_size = 2 # Here we define the context window size\n",
    "training_data = generate_training_data(words, word_to_idx, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93841b",
   "metadata": {},
   "source": [
    "### Define the Skip-gram Model\n",
    "Raphael Steinborn 351421 | Vineeth Racharla 349073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cded71f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)  # Input word embeddings\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)  # Output word embeddings\n",
    "    \n",
    "    def forward(self, center_word, context_word, negative_samples):\n",
    "        # Get embeddings for center word, context word, and negative samples\n",
    "        center_embed = self.in_embeddings(center_word).unsqueeze(1)\n",
    "        context_embed = self.out_embeddings(context_word).unsqueeze(1)\n",
    "        neg_embed = self.out_embeddings(negative_samples)\n",
    "        \n",
    "        # Calculate positive score\n",
    "        pos_score = torch.bmm(context_embed, center_embed.transpose(1, 2)).squeeze()\n",
    "        \n",
    "        # Calculate negative score\n",
    "        neg_score = torch.bmm(neg_embed, center_embed.transpose(1, 2)).squeeze()\n",
    "        \n",
    "        # Calculate positive and negative loss\n",
    "        pos_loss = torch.log(torch.sigmoid(pos_score)).sum()\n",
    "        neg_loss = torch.log(torch.sigmoid(-neg_score)).sum()\n",
    "        \n",
    "        return -(pos_loss + neg_loss)  # Return total loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca07a31",
   "metadata": {},
   "source": [
    "### Implement Negative Sampling\n",
    "Raphael Steinborn 351421 | Lidia Makishti 346621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42bc016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(target, vocab_size, k=5):\n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < k:\n",
    "        sample = random.randint(0, vocab_size - 1)\n",
    "        if sample != target:\n",
    "            neg_samples.append(sample)\n",
    "    return torch.tensor(neg_samples)  # Convert the list of negative samples to a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d3e90",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Raphael Steinborn 351421| Saqib Sarwar 351757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08ad3a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [54:09<00:00, 414.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 24215613.40907107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [58:05<00:00, 386.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 15406641.4315007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [58:39<00:00, 382.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 9858149.241396632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [58:21<00:00, 384.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 6761020.5883722305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [58:37<00:00, 382.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 5369496.472452118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [1:01:24<00:00, 365.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 4566359.622814983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [1:01:34<00:00, 364.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 4035564.3589102156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [1:00:44<00:00, 369.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 3641700.801302188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [55:30<00:00, 404.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 3342709.3068018933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1346322/1346322 [55:41<00:00, 402.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 3092290.2434660653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100 # Here we define the embedding dimension\n",
    "learning_rate = 0.01 # Here we define the learning rate\n",
    "epochs = 10 # Here we define the amount of epochs\n",
    "\n",
    "model = SkipGramModel(vocab_size, embedding_dim).to(device) # Move model to device\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    random.shuffle(training_data)\n",
    "    for target, context in tqdm(training_data):\n",
    "        # Move data to the same device as the model\n",
    "        target_tensor = torch.tensor([target], dtype=torch.long).to(device)\n",
    "        context_tensor = torch.tensor([context], dtype=torch.long).to(device)\n",
    "        neg_samples_tensor = get_negative_samples(target, vocab_size, k=5).unsqueeze(0).to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = model(target_tensor, context_tensor, neg_samples_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch: {epoch}, Loss: {total_loss}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598246b7",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "Raphael Steinborn 351421 | Lidia Makishti 346621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f4e6da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for \"economy\":\n",
      "[ 0.5192357  -0.12822421  0.01935717  0.08980225 -0.30135232  0.12298822\n",
      "  0.27826655 -0.07729244 -0.1602444   0.12366064 -0.09298617  0.27902424\n",
      "  0.13965435 -0.03480436 -0.35620016 -0.108164    0.11556055 -0.20788234\n",
      "  0.11945654 -0.3874983  -0.3050467  -0.09299957  0.28377932 -0.11577014\n",
      "  0.17916234  0.05045917 -0.3179455  -0.21785542  0.28134823  0.30142704\n",
      "  0.24886775  0.17752081 -0.00576534 -0.13229336  0.09931571  0.00523115\n",
      "  0.0993351  -0.04925102  0.13245218 -0.0794709   0.24092875 -0.28034765\n",
      " -0.20903786 -0.08777105 -0.04629131  0.14778823 -0.11245999  0.02697894\n",
      " -0.07635371 -0.01300771  0.00153163  0.0284095   0.28540838  0.43226308\n",
      " -0.03276398 -0.00552599  0.00658814 -0.22874175  0.10414997  0.42934507\n",
      " -0.06483742 -0.03806027  0.03490882  0.01837328  0.27242014  0.05900625\n",
      " -0.09647197 -0.04939486  0.04505193  0.10131337  0.13005303 -0.28051662\n",
      " -0.15064062 -0.23593275 -0.07970688 -0.14525747 -0.31812096 -0.12576556\n",
      " -0.38688025 -0.08109877  0.09819821  0.28448892  0.14910893  0.20182255\n",
      " -0.2891947   0.07305079  0.01338574 -0.0316611  -0.06929386 -0.24462707\n",
      " -0.02664411  0.14390215 -0.12537368  0.14021432  0.07123468  0.02829666\n",
      " -0.06855819 -0.15545148 -0.13639182 -0.2905639 ]\n",
      "Similarity between \"economy\" and \"market\": 0.6592734734797076\n",
      "Similarity between \"economy\" and \"banana\": 0.10109470428504075\n"
     ]
    }
   ],
   "source": [
    "def get_word_embedding(word):\n",
    "    word_idx = word_to_idx[word]\n",
    "    word_tensor = torch.tensor([word_idx], dtype=torch.long).to(device) # Move tensor to device\n",
    "    return model.in_embeddings(word_tensor).detach().cpu().numpy().flatten() # Move embedding back to CPU for NumPy\n",
    "\n",
    "# Example word embeddings\n",
    "word = 'economy'\n",
    "embedding = get_word_embedding(word)\n",
    "print(f'Embedding for \"{word}\":\\n{embedding}')\n",
    "\n",
    "# Evaluate using a standard word similarity benchmark (e.g., SimLex-999)\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def word_similarity(word1, word2):\n",
    "    emb1 = get_word_embedding(word1)\n",
    "    emb2 = get_word_embedding(word2)\n",
    "    return 1 - cosine(emb1, emb2)\n",
    "\n",
    "# Example word similarities\n",
    "print(f'Similarity between \"economy\" and \"market\": {word_similarity(\"economy\", \"market\")}')\n",
    "print(f'Similarity between \"economy\" and \"banana\": {word_similarity(\"economy\", \"banana\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86d1b7",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "The reuters dataset is well suited for the task, however, because of our limited computational resources, it was not possible for us to try different hyperparameters (window size, embeddings, epochs). Therefore we have not implemented Phrase Learning or further analogical reasoning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
